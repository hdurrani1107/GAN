# Empirical Exploration of Generative Adversarial Networks

**Authors:** Humzah Durrani, Zoe Perkins  
**Affiliation:** Boston University  
**Contact:** hhd8@bu.edu, perkinsz@bu.edu

---

## Overview

This repository contains all code, data preprocessing scripts, and example outputs for our empirical study comparing three generative models—Vanilla GAN (VGAN), Deep Convolutional GAN (DCGAN), and Variational Autoencoder (VAE)—on image generation tasks using the CelebA dataset. Our goals were to:

- Tune hyperparameters (noise dimension, learning rate) for a VGAN and assess their impact.  
- Evaluate how training dataset size (2.5 k, 5 k, 10 k images) affects VGAN performance.  
- Compare VGAN, DCGAN, and VAE in terms of FID, IS, and visual fidelity.

Dataset used: CelebA
https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

*This study serves as a practical guide for deploying GANs under limited computational resources.*
*Our results can be viewed on our paper*
